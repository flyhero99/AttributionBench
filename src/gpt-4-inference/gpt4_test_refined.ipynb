{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T09:55:05.223929Z",
     "start_time": "2023-06-22T09:54:59.784451Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "with open(\"../../openai_api_key.txt\", \"r\") as f:\n",
    "    openai.api_key = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5bd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message = \\\n",
    "# \"\"\"\n",
    "# ### Instruction:\\n\n",
    "# Please verify whether the reference can support the answer to the question.\n",
    "# Options: 'attributable' or 'not attributable'.\n",
    "# Note that, there are usually two criteria to judge the answer: (i) informativeness: whether the\n",
    "# claim provides a clear and direct answer that can clearly answer the question, and (ii) \n",
    "# attributability: whether the explanation is attributable to the references, which means there are no factual errors\n",
    "# in the claim that is not supported or conflict to the references.\n",
    "# Your task is to judge the attributability, instead of the informativeness, and you must not confuse them.\n",
    "# \\n\\n\n",
    "# ### Input:\n",
    "# \"\"\"\n",
    "instruction = \"### Instruction:\\nPlease verify whether the reference can support the claim to the question. Options: 'attributable' or 'not attributable'.\\n\"\n",
    "input = \"### Input:\\nQuestion: {}\\n\\nClaim: {}\\n\\nReference: {}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dac2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_subset_name = \"test\"\n",
    "test_subset_name = \"test_all_subset_balanced\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "# model = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "# mode = \"few_shot\"\n",
    "mode = \"zero_shot\"\n",
    "\n",
    "# test_data = [row for row in load_dataset(\"osunlp/AttributionBench\", split=test_subset_name)][:1000]\n",
    "test_data = [json.loads(l) for l in open(\"../../data/hf_data/test_all_subset_balanced.jsonl\")]\n",
    "\n",
    "# TASK_PROMPTS = {p['task_name']:[p['prompt_template'],p['input_template'],p['label_map']] for p in json.load(open(\"task_prompts.json\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768945b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'who wrote the theme song for mission impossible',\n",
       " 'claim': 'Lalo Schifrin',\n",
       " 'claim_raw_string': 'Lalo Schifrin',\n",
       " 'response': 'Lalo Schifrin',\n",
       " 'references': ['Title: Mission: Impossible (1966 TV series)\\nSection: Inspirations and innovations\\n\\nMission: Impossible is still recognized for its innovative use of music. Composer Lalo Schifrin wrote several distinctive pieces for the series. The visual cuts in the main title sequence were timed to the beats and measures of the theme tune—written in (unusual) 54 time—while an animated burning fuse moved across the screen. Most episodes included fairly long dialogue-free sequences showing the team members—particularly electronics expert Barney Collier—making technical preparations for the mission, usually to the accompaniment of another easily recognizable tune called \"The Plot.\" Lalo Schifrin also wrote a theme piece for each main character and the sound track for each episode incorporated variations of these throughout. Even when an episode\\'s score is credited to some other composer, Desilu\\'s music supervisor Jack Hunsacker would re-edit it, adding Schifrin melodies from the library. The series had great impact on film and TV music. Before Mission: Impossible, a common compliment was along the lines of \"the score worked very well but never got in the way or called attention to itself.\" By contrast, Mission: Impossible was praised for the prominence of its music.'],\n",
       " 'citation_links': [],\n",
       " 'webpage_references': [],\n",
       " 'attribution_label': 'attributable',\n",
       " 'src_dataset': 'AttributedQA',\n",
       " 'id': 'AttributedQA_c9389aef-207d-464a-97bd-16601544bc85'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8990278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example, mode=\"zero_shot\"):\n",
    "    task_prompt = \"### Instruction:\\nPlease verify whether the reference can support the claim to the question. Options: 'attributable' or 'not attributable'.\\n\"\n",
    "    input_template = \"### Input:\\nQuestion: {}\\n\\nClaim: {}\\n\\nReference: {}\\n\\n### Response:\"\n",
    "\n",
    "    # \"input_template\": \"Question: {}\\n\\nClaim: {}\\n\\nResponse: {}\\n\\nReference: {}\\n\",\n",
    "    query = example['question'] if example['question'] and example['question'] not in [\"nan\", \"\", None] else \"\"\n",
    "    answer = example['claim'] if example['claim'] and example['claim'] not in [\"nan\", \"\", None] else \"\"\n",
    "    response = example['response'] if example['response'] and example['response'] not in [\"nan\", \"\", None] else \"\"\n",
    "    documents_concatenation = \"\\n\\n\\n\".join(example[\"references\"])\n",
    "    input = input_template.format(query, answer, documents_concatenation)\n",
    "    \n",
    "    prompt = \"{}{}\".format(task_prompt, input)\n",
    "    \n",
    "    return prompt\n",
    "    # return prompt if len(prompt) < 40000 else prompt[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccaa84ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Please verify whether the reference can support the claim to the question. Options: 'attributable' or 'not attributable'.\n",
      "### Input:\n",
      "Question: who wrote the theme song for mission impossible\n",
      "\n",
      "Claim: Lalo Schifrin\n",
      "\n",
      "Reference: Title: Mission: Impossible (1966 TV series)\n",
      "Section: Inspirations and innovations\n",
      "\n",
      "Mission: Impossible is still recognized for its innovative use of music. Composer Lalo Schifrin wrote several distinctive pieces for the series. The visual cuts in the main title sequence were timed to the beats and measures of the theme tune—written in (unusual) 54 time—while an animated burning fuse moved across the screen. Most episodes included fairly long dialogue-free sequences showing the team members—particularly electronics expert Barney Collier—making technical preparations for the mission, usually to the accompaniment of another easily recognizable tune called \"The Plot.\" Lalo Schifrin also wrote a theme piece for each main character and the sound track for each episode incorporated variations of these throughout. Even when an episode's score is credited to some other composer, Desilu's music supervisor Jack Hunsacker would re-edit it, adding Schifrin melodies from the library. The series had great impact on film and TV music. Before Mission: Impossible, a common compliment was along the lines of \"the score worked very well but never got in the way or called attention to itself.\" By contrast, Mission: Impossible was praised for the prominence of its music.\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "print(format_prompt(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff90f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def get_attr_from_chatgpt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                top_p=0.9,\n",
    "                max_tokens=512,\n",
    "                n=1\n",
    "            )\n",
    "            # print(response)\n",
    "            time.sleep(1)\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", str(e))\n",
    "            # 打印详细的堆栈跟踪信息\n",
    "            traceback.print_exc()\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f04717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1525\n",
      "### Instruction:\n",
      "Please verify whether the reference can support the claim to the question. Options: 'attributable' or 'not attributable'.\n",
      "### Input:\n",
      "Question: who wrote the theme song for mission impossible\n",
      "\n",
      "Claim: Lalo Schifrin\n",
      "\n",
      "Reference: Title: Mission: Impossible (1966 TV series)\n",
      "Section: Inspirations and innovations\n",
      "\n",
      "Mission: Impossible is still recognized for its innovative use of music. Composer Lalo Schifrin wrote several distinctive pieces for the series. The visual cuts in the main title sequence were timed to the beats and measures of the theme tune—written in (unusual) 54 time—while an animated burning fuse moved across the screen. Most episodes included fairly long dialogue-free sequences showing the team members—particularly electronics expert Barney Collier—making technical preparations for the mission, usually to the accompaniment of another easily recognizable tune called \"The Plot.\" Lalo Schifrin also wrote a theme piece for each main character and the sound track for each episode incorporated variations of these throughout. Even when an episode's score is credited to some other composer, Desilu's music supervisor Jack Hunsacker would re-edit it, adding Schifrin melodies from the library. The series had great impact on film and TV music. Before Mission: Impossible, a common compliment was along the lines of \"the score worked very well but never got in the way or called attention to itself.\" By contrast, Mission: Impossible was praised for the prominence of its music.\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "prompt = format_prompt(test_data[1], mode=mode)\n",
    "print(len(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c70f68b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb 单元格 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# prompt = format_prompt(test_data[91], prompt_type='attribution-binary-with-whole-response-and-informativeness-definition', input_has_query=True, input_has_response=True, mode=mode)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m prompt \u001b[39m=\u001b[39m format_prompt(test_data[\u001b[39m1\u001b[39m], mode\u001b[39m=\u001b[39mmode)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m response \u001b[39m=\u001b[39m get_attr_from_chatgpt(prompt, model)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mprompt:\u001b[39m\u001b[39m\"\u001b[39m, prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresponse:\u001b[39m\u001b[39m\"\u001b[39m, response)\n",
      "\u001b[1;32m/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb 单元格 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m             model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m             messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m             temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m             top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m             max_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m             n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# print(response)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmy2/ML-A100/home/xiangyue/lyf/AttributionBench/src/gpt-4-inference/gpt4_test_refined.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    359\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prompt = format_prompt(test_data[91], prompt_type='attribution-binary-with-whole-response-and-informativeness-definition', input_has_query=True, input_has_response=True, mode=mode)\n",
    "prompt = format_prompt(test_data[1], mode=mode)\n",
    "response = get_attr_from_chatgpt(prompt, model)\n",
    "print(\"prompt:\", prompt)\n",
    "print(\"response:\", response)\n",
    "print(\"ground_truth\", test_data[1][\"attribution_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb6c3f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gpt-4_expertqa_stanford_strlen_within20k_random500_zero_shot_old_document_result.json\n"
     ]
    }
   ],
   "source": [
    "output_file = \"./{}_{}_{}_old_document_result.json\".format(model, test_subset_name, mode)\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e421fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a40d3227fee413298b1f0253e02cf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#downsample to save cost\n",
    "\n",
    "# random.shuffle(test_data)\n",
    "# test_data = test_data[:500]\n",
    "\n",
    "prompt_list = []\n",
    "\n",
    "# for task_name in [\"attribution-binary-with-whole-response-and-informativeness-definition\"]:\n",
    "for task_name in [\"attribution-binary-with-definition\"]:\n",
    "    res_key = '{}.eval.{}'.format(model,task_name)\n",
    "\n",
    "    for example in tqdm(test_data):\n",
    "        prompt = format_prompt(example, prompt_type=task_name, webpage_documents=True, input_has_query=True, input_has_response=False, mode=mode)\n",
    "        prompt_list.append(prompt)\n",
    "        example[res_key] = get_attr_from_chatgpt(prompt,model)\n",
    "\n",
    "    json.dump(test_data, open(output_file,'a'))\n",
    "\n",
    "# print(np.mean([len(x) for x in prompt_list]))\n",
    "# print(np.std([len(x) for x in prompt_list]))\n",
    "# print(np.median([len(x) for x in prompt_list]))\n",
    "# print(np.max([len(x) for x in prompt_list]))\n",
    "# print(np.min([len(x) for x in prompt_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d94bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T09:55:17.373705Z",
     "start_time": "2023-06-22T09:55:17.219660Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_few_shot_demo(prompt_type):\n",
    "    demo_file_name = {\n",
    "        \"attribution-no-definition\": \"demo_attr.txt\",\n",
    "        \"attribution-with-definition\": \"demo_attr.txt\",\n",
    "        \"fact-checking\": \"demo_fact-checking.txt\",\n",
    "        \"nli\": \"demo_NLI.txt\",\n",
    "        \"summarization\": \"demo_sum.txt\"\n",
    "    }\n",
    "    with open(f\"../few-shot-demo/{demo_file_name[prompt_type]}\") as rf:\n",
    "        demo_str = rf.read()\n",
    "        rf.close()\n",
    "    return demo_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221e8d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T09:55:23.741898Z",
     "start_time": "2023-06-22T09:55:23.415248Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_prompt(example, prompt_type = \"attribution-binary-with-definition\", input_has_query = True, mode=\"zero_shot\"):\n",
    "    task_prompt, input_template, _ = TASK_PROMPTS[prompt_type]\n",
    "    if input_has_query:\n",
    "        query = example['question'] if example['question'] and example['question'] not in [\"nan\",\"\"] else \"\"\n",
    "        answer = example['claim'] if example['claim'] and example['claim'] not in [\"nan\",\"\"] else \"\"\n",
    "        input = input_template.format(\"Question: \" + query + \" \" + \"Answer: \" + answer, \"Reference: \" + example['documents_concatenation'])\n",
    "\n",
    "    else:\n",
    "        answer = example['claim'] if example['claim'] and example['claim'] not in [\"nan\",\"\"] else \"\"\n",
    "        input = input_template.format(\"Answer: \" + answer, \"Reference: \" + example['documents_concatenation'])\n",
    "    \n",
    "    if mode==\"few_shot\":\n",
    "        demo_str = read_few_shot_demo(prompt_type)\n",
    "        prompt = \"\\n{}\\n{}\\n\\n### Input: \\n{}\\n### Response:\".format(task_prompt,demo_str,input)\n",
    "    else:\n",
    "        prompt = \"\\n{}\\n\\n### Input: \\n{}\\n### Response:\".format(task_prompt,input)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09caff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T10:37:20.996358Z",
     "start_time": "2023-06-22T10:37:20.945662Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_attr_from_chatgpt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages=[\n",
    "        # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                top_p=0.9,\n",
    "                max_tokens=512,\n",
    "                n=1\n",
    "            )\n",
    "            # print(response)\n",
    "            time.sleep(0.75)\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981e805",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T10:37:26.687380Z",
     "start_time": "2023-06-22T10:37:25.088167Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = format_prompt(test_data[91],'attribution-binary-with-definition',input_has_query=True,mode=mode)\n",
    "response = get_attr_from_chatgpt(prompt,model)\n",
    "print(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T09:55:49.823802Z",
     "start_time": "2023-06-22T09:55:49.762040Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = \"./{}_{}_{}_result.json\".format(model,test_subset_name,mode)\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T10:06:46.099623Z",
     "start_time": "2023-06-22T09:55:57.428922Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#downsample to save cost\n",
    "\n",
    "# random.shuffle(test_data)\n",
    "# test_data = test_data[:500]\n",
    "\n",
    "for task_name in [\"attribution-binary-with-definition\"]:\n",
    "    res_key = '{}.eval.{}'.format(model,task_name)\n",
    "\n",
    "    for example in tqdm(test_data):\n",
    "        prompt = format_prompt(example,prompt_type=task_name,mode=mode)\n",
    "        example[res_key] = get_attr_from_chatgpt(prompt,model)\n",
    "\n",
    "    json.dump(test_data, open(output_file,'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cbf48d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T10:08:25.279173Z",
     "start_time": "2023-06-22T10:08:25.176923Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_pred_label(prediction, prompt_type=\"attribution-binary-with-whole-response-and-informativeness-definition\"):\n",
    "    label_map = TASK_PROMPTS[prompt_type][-1]\n",
    "    label_regex = r\"|\".join(list(label_map.keys()))\n",
    "\n",
    "    pred_label = re.search(label_regex, prediction, re.IGNORECASE).group() if re.search(\n",
    "            label_regex,\n",
    "            prediction, re.IGNORECASE) is not None else 'None'\n",
    "\n",
    "    pred_label = label_map[pred_label.lower()] if pred_label.lower() in label_map else \"None\"\n",
    "\n",
    "    return pred_label\n",
    "\n",
    "# extract_pred_label(test_data[1][res_key], prompt_type=\"attribution-binary-with-definition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf36908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T10:08:26.999181Z",
     "start_time": "2023-06-22T10:08:26.922200Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_confusion_matrix(confusion_matrix):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1 = np.zeros(num_classes)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        true_positives = confusion_matrix[i, i]\n",
    "        false_positives = np.sum(confusion_matrix[:, i]) - true_positives\n",
    "        false_negatives = np.sum(confusion_matrix[i, :]) - true_positives\n",
    "\n",
    "        precision[i] = true_positives / (true_positives + false_positives)\n",
    "        recall[i] = true_positives / (true_positives + false_negatives)\n",
    "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "\n",
    "    micro_true_positives = np.sum(np.diag(confusion_matrix))\n",
    "    micro_false_positives = np.sum(confusion_matrix, axis=0) - np.diag(confusion_matrix)\n",
    "\n",
    "    micro_f1 = micro_true_positives / (micro_true_positives + np.sum(micro_false_positives))\n",
    "    macro_f1 = np.mean(f1)\n",
    "\n",
    "    return precision, recall, f1, micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9748cbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gpt-4_expertqa_stanford_strlen_within20k_random500_zero_shot_new_document_result.json'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380850de",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "960f0ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-22T10:08:28.020870Z",
     "start_time": "2023-06-22T10:08:27.895356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attribution-binary-with-definition\n",
      "[[597 165]\n",
      " [102 136]]\n",
      "Accuracy: 0.733\n",
      "Precision: [0.85407725 0.45182724]\n",
      "Recall: [0.78346457 0.57142857]\n",
      "F1: [0.81724846 0.50463822]\n",
      "micro_f1: 0.733\n",
      "macro_f1: 0.6609433394414327\n"
     ]
    }
   ],
   "source": [
    "# for task_name in [\"attribution-binary-with-whole-response-and-informativeness-definition\"]:\n",
    "for task_name in [\"attribution-binary-with-definition\"]:\n",
    "    res_key = '{}.eval.{}'.format(model, task_name)\n",
    "    pred_labels = [extract_pred_label(example[res_key], prompt_type=task_name) for example in json.load(open(output_file))]\n",
    "    true_labels = [example['attribution_label'] for example in json.load(open(output_file))]\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    conf_matrix = confusion_matrix(true_labels, pred_labels, labels=[\"attributable\", \"not attributable\"])\n",
    "\n",
    "    precision, recall, f1, micro_f1, macro_f1 = evaluate_confusion_matrix(conf_matrix)\n",
    "\n",
    "    print(task_name)\n",
    "    print(conf_matrix)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"micro_f1:\", micro_f1)\n",
    "    print(\"macro_f1:\", macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([example for example in json.load(open(output_file)) if example[\"src_dataset\"] in [\"hagrid_dev\", \"hagrid_train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([example for example in json.load(open(output_file)) if example[\"attribution_label\"] == \"attributable\"]))\n",
    "print(len([example for example in json.load(open(output_file)) if example[\"attribution_label\"] == \"not attributable\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e97e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 108\n",
    "dev = 166\n",
    "hagrid_train_acc = 0.81\n",
    "hagrid_dev_acc = 0.75\n",
    "hagrid_acc = (hagrid_train_acc * train + hagrid_dev_acc * dev) / (train + dev)\n",
    "print(hagrid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4f801fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expertqa_old = json.load(open(\"/research/nfs_sun_397/li.14042/projects/attribution-eval/code/gpt-4_expertqa_strlen_within20k_random500_zero_shot_old_document_result.json\"))\n",
    "stanford_old = json.load(open(\"/research/nfs_sun_397/li.14042/projects/attribution-eval/code/gpt-4_stanford_strlen_within20k_random500_zero_shot_stanford_old_document_result.json\"))\n",
    "old = expertqa_old + stanford_old\n",
    "json.dump(old, open(\"/research/nfs_sun_397/li.14042/projects/attribution-eval/code/gpt-4_expertqa_stanford_strlen_within20k_random500_zero_shot_old_document_result.json\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05781748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
