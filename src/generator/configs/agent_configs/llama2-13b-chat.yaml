agent_class: hf
parameters:
    name: llama2-13b-chat
    model_name: meta-llama/Llama-2-13b-chat-hf
    prompt_name: llama-2

    batch_size: 32

    # you can modify those generation config below
    do_sample: True
    temperature: 1
    top_p: 0.9
    max_new_tokens: 64
    min_new_tokens: 4